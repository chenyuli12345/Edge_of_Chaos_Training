{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse #argparse是用来处理命令行参数的工具，提供了一种方便的方式来定义用户应该如何指定/处理这些参数\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras #keras是tf上的一个高级接口，用于构建和训练深度学习模型\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #这是一个损失函数，常用于多分类问题，是交叉熵损失函数的一种（适用于真实标签是整数的情况，函数会将真实标签视为类别索引，并使用这个索引从预测的概率分布中选择一个概率）\n",
    "#例如，如果你有一个分类问题，类别有 3 类，那么你的真实标签可能是 0、1 或 2。你的模型会预测出一个概率分布，如 [0.1, 0.2, 0.7]，这表示模型认为样本属于第 0 类、第 1 类和第 2 类的概率分别是 0.1、0.2 和 0.7。\n",
    "import numpy as np\n",
    "\n",
    "#下面的包均为此目录下的程序\n",
    "import utils\n",
    "import loggingreporter\n",
    "import plot_figure2\n",
    "import plot_figure4_5\n",
    "import plot_figure6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "#首先创建了一个argparse.ArgumentParser对象，这是Python的命令行参数解析库argparse的核心部分。argparse.ArgumentParser类用于处理命令行参数。它会从命令行读取参数，然后将它们转换为 Python 中的适当类型，最后将这些参数传递给你的程序。这使得你的程序可以以一种用户友好的方式接受命令行输入。\n",
    "#创建argparse.ArgumentParser对象后，你可以使用它的add_argument方法添加命令行参数，然后使用parse_args方法解析命令行参数。\n",
    "#本例将创建的argparse.ArgumentParser对象赋值给变量parser。ArgumentParser的构造函数接受多个参数，其中一个是 description，它是一个字符串，用于描述这个命令行程序的目的。在这个例子中，description 被设置为 'Asymptotic stability study of SGD'，这意味着这个程序的目的是研究 SGD（随机梯度下降）的渐近稳定性。\n",
    "parser = argparse.ArgumentParser(description='Asymptotic stability study of SGD')\n",
    "#添加命令行参数，type用于指定参数的类型；default用于指定参数的默认值（命令行不提供该参数的情况下）；help用于指定当用户请求帮助时显示的文本；metavar是元变量，在帮助信息中使用参数值的名称；dest是目标，存储该参数值的属性的名称（未提供时argparse会根据参数的名称自动生成一个）\n",
    "parser.add_argument('--dataset', type=str, default='mnist',\n",
    "                    help=\"datset {'mnist', 'kmnist', 'emnist/mnist'}. default: 'mnist'\")\n",
    "#例如，此命令添加了一个名为--dataset的参数，类型是字符串，默认值为‘mnist’并且有一个帮助信息\n",
    "parser.add_argument('--activation-func', type=str, default='relu',\n",
    "                    help='activation function for hidden layers')\n",
    "parser.add_argument('--epochs', default=4, type=int, metavar='N',\n",
    "                    help='number of total epochs to run, should > 3')\n",
    "parser.add_argument('--batch-size', default=32, type=int, metavar='N',\n",
    "                    help='batch size for training')\n",
    "parser.add_argument('--optimizer', type=str, default='SGD',\n",
    "                    help='optimizer used for training')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.01, type=float,\n",
    "                    metavar='LR', help='initial learning rate', dest='lr')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum for SGD')\n",
    "parser.add_argument('--beta-1', default=0.9, type=float, metavar='M',\n",
    "                    help='beta_1 in Adam')\n",
    "parser.add_argument('--beta-2', default=0.999, type=float, metavar='M',\n",
    "                    help='beta_2 in Adam')\n",
    "parser.add_argument('--weight-decay', default=0, type=float,\n",
    "                    metavar='W', help='weight decay (default: 0)',\n",
    "                    dest='weight_decay')\n",
    "parser.add_argument('--num-repeats', type=int, default=10,\n",
    "                    help='number of simulation repeats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time() \n",
    "    args = parser.parse_args() #使用argparse库解析命令行参数的方式。parse_args() 方法会读取命令行参数，将它们转换为适当的类型，然后返回一个命名空间（这里是args），这个命名空间包含了所有的命令行参数。\n",
    "    args.log_epochs = np.arange(args.epochs) #创建一个新属性log_epochs，是一个从0到args.epochs-1的数组，可以用于记录训练过程中的每个epoch\n",
    "    #创建新属性arguments，是一个字符串，包含了所有命令行参数，可以用于生成文件路径\n",
    "    args.arguments = '{}_{}_{}/{}_{}_{}_{}_{}_{}_{}'.format(args.dataset, args.optimizer, args.activation_func, args.epochs, int(args.lr*10000), int(\n",
    "        args.batch_size), int(args.momentum*1000), int(args.beta_1*1000), int(args.beta_2*10000000), int(args.weight_decay*1000000))\n",
    "    #在创建三个新的属性，均为字符串，用于保存权重、损失和accuracy的文件路径\n",
    "    args.save_weights_dir = 'rawdata/weights/{}'.format(args.arguments)\n",
    "    args.save_losses_dir = 'rawdata/losses/{}'.format(args.arguments)\n",
    "    args.save_scores_dir = 'rawdata/scores/{}'.format(args.arguments)\n",
    "\n",
    "    # (x_train, y_train), (x_test, y_test) = utils.load_qmnist_data()\n",
    "    (x_train, y_train), (x_test, y_test) = utils.load_data(args.dataset)\n",
    "    args.input_shape = x_train.shape[1]\n",
    "\n",
    "    for num_repeat in range(args.num_repeats):\n",
    "        # break\n",
    "        print('num_repeat={}'.format(num_repeat))\n",
    "        args.save_weights_dir = 'rawdata/weights/{}/{}'.format(\n",
    "            args.arguments, num_repeat)\n",
    "        args.save_losses_dir = 'rawdata/losses/{}/{}'.format(\n",
    "            args.arguments, num_repeat)\n",
    "        args.save_scores_dir = 'rawdata/scores/{}/{}'.format(\n",
    "            args.arguments, num_repeat)\n",
    "\n",
    "        if args.activation_func == 'relu':\n",
    "            activation_func = tf.nn.relu\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(args.input_shape, activation=activation_func, name='layer_1',\n",
    "                                      use_bias=False, input_shape=(args.input_shape,),\n",
    "                                      kernel_regularizer=keras.regularizers.l2(args.weight_decay),),\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.softmax,\n",
    "                                      kernel_regularizer=keras.regularizers.l2(args.weight_decay))\n",
    "            ])\n",
    "        elif args.activation_func == 'tanh':\n",
    "            activation_func = tf.nn.tanh\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(args.input_shape, activation=activation_func, name='layer_1',\n",
    "                                      kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                          mean=(0)/(args.input_shape), stddev=1/(2*np.sqrt(args.input_shape))),\n",
    "                                      use_bias=False, input_shape=(args.input_shape,),\n",
    "                                      kernel_regularizer=keras.regularizers.l2(args.weight_decay),),\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.softmax,\n",
    "                                      # kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                                      #     mean=(0)/(args.input_shape), stddev=1/(2*np.sqrt(args.input_shape))),\n",
    "                                      kernel_regularizer=keras.regularizers.l2(args.weight_decay))\n",
    "            ])\n",
    "\n",
    "        if args.optimizer == 'SGD':\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                learning_rate=args.lr, momentum=args.momentum, nesterov=False, name='SGD')\n",
    "        elif args.optimizer == 'Adam':\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=args.lr, beta_1=args.beta_1, beta_2=args.beta_2, epsilon=1e-07,\n",
    "                amsgrad=False, name='Adam')\n",
    "        metric_loss = SparseCategoricalCrossentropy(from_logits=False,\n",
    "                                                    name='sparse_categorical_crossentropy')\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=[metric_loss, 'accuracy'])\n",
    "        reporter = loggingreporter.LoggingReporter(\n",
    "            args, x_train, y_train, x_test, y_test)\n",
    "        # model.fit(x_train, y_train, epochs=args.epochs,\n",
    "        #           verbose=0, callbacks=[reporter, ], validation_split=0.2)\n",
    "        model.fit(x_train, y_train, epochs=args.epochs, batch_size=args.batch_size,\n",
    "                  verbose=0, callbacks=[reporter, ], validation_data=(x_test, y_test))\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plot Fig.2 and Fig.3(a) in the paper.\n",
    "    # For Fig.3(b)-(d), you need to record the slope at Fig.3(a) for different\n",
    "    # hyperparameters and use the file './coefficient(figure3).py' to plot Fig.3(b)-(d).\n",
    "    plot_figure2.plot_weights_path(args)\n",
    "    plot_figure2.plot_loss_acc(args)\n",
    "    plot_figure2.plot_weights_var_mean(args)\n",
    "\n",
    "    # Plot Fig.4-5 in the paper\n",
    "    # plot_figure4_5.plot_rescale_same(args)\n",
    "    # plot_figure4_5.plot_rescale_different(args)\n",
    "\n",
    "    # Plot Fig.6 in the paper\n",
    "    # plot_figure6.plot_all_loss_acc(args)\n",
    "    # plot_figure6.plot_weight_decay_path(args)\n",
    "    # plot_figure6.plot_weight_decay_var_mean(args)\n",
    "    # plot_figure6.plot_weight_decay_loss_acc(args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('elapsed time is {} mins'.format((end_time-start_time)/60))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
